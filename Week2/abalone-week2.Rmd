---
title: "Abalone"
author: "Raghu"
date: "4 september "  
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

To clean the Environment
```{r}
rm(list = ls(all=TRUE))
```
TO make sure that the files we read exist in the workig directory or not
```{r}
setwd("C:/Users/RaghuTeja/Desktop/intern/")
```
TO read the data 
```{r}
mydata <- read.table("abalone.txt", header=FALSE, sep=",")
```
TO know the first 6 rows of the dataset
```{r}
head(mydata)
```
To know the Last 6 rows in the data
```{r}
tail(mydata)
```
To know the structure of the data.
```{r}
str(mydata)
```
To check for any NA values in the entire dataset
```{r}
sum(is.na(mydata))
```
To find the detailed information of each any every attribute.
```{r}
summary(mydata)
```

```{r}
table(mydata$V1)

```

TO check the Standard Deviation in each attribute
```{r}
sd(mydata$V2)
sd(mydata$V3)
sd(mydata$V4)
sd(mydata$V5)
sd(mydata$V6)
sd(mydata$V7)
sd(mydata$V8)
sd(mydata$V9)

```
To know the no: of unique values in each attribute
```{r}
unique(mydata$V1)
unique(mydata$v2)
unique(mydata$V3)
unique(mydata$V4)
unique(mydata$V5)
unique(mydata$V6)
unique(mydata$V7)
unique(mydata$V8)
unique(mydata$V9)
```


```{r}
target<-mydata$V1
mydata$V1<-NULL
```

To get the correlation matrix for the numerical attributes
```{r}
res <- cor(mydata)
round(res, 2)
```
TO see the correlation between the attributes graphically
```{r}

library(corrplot)

corrplot(cor(mydata), method = "number")
```


mydata$V3=NULL
mydata$V6=NULL
mydata$V7=NULL

```{r}
mydata = cbind(mydata,target)
```

To check for the outliers in the data.
```{r}
boxplot(mydata
        , main="With outliers")
```

```{r}
set.seed(100)

# the "sample()" function helps us to randomly sample 70% of the row indices of the dataset

train_rows <- sample(x = 1:nrow(mydata), size = 0.7*nrow(mydata))

# We use the above indices to subset the train and test sets from the data

train_data <- mydata[train_rows, ]

test_data <- mydata[-train_rows, ]
    
```

```{r}
y=train_data$target

```
Standardize the entire dataset excluding target variable on both test and train
```{r}
library(vegan)
train_data$target=NULL
train_data=decostand(train_data,"range")
train_data=cbind(train_data,y)

z=test_data$target
test_data$target=NULL
test_data=decostand(test_data,"range") 
test_data=cbind(test_data,z)
```

```{r}
table(mydata$target)
test_data$z<-as.factor(test_data$z)
train_data$y<-as.factor(train_data$y)
```
Sclaed PCA computation

* Use the prcomp() function to get the scaled principle components as it has two additional arguments to do so

* Remove the target variable while doing so
```{r}
pca_scaled <- prcomp(train_data[, !(names(train_data) %in% c("y"))], center = T, scale. = T)

```

```{r}
plot(pca_scaled)
```
```{r}
biplot(pca_scaled)
```

Apply PCA on the Original Data
```{r}

train_pca_e <- as.data.frame(predict(pca_scaled[,], train_data[, !(names(train_data) %in% c("y"))]))

test_pca_e <- as.data.frame(predict(pca_scaled, test_data[, !(names(train_data) %in% c("z"))]))

```